# Neural Network Compression

This project explores and compares three different model compression techniques on the MNIST dataset:

- **Knowledge Distillation**
- **Structured Pruning**
- **Unstructured Pruning**

The goal is to evaluate the trade-offs between model size, inference time, and performance (accuracy).  
All experiments and results are documented in the notebook `models_evaluation.ipynb`.

This work serves as a comparative analysis of how different compression methods affect neural network efficiency in practical scenarios.
